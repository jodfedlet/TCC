{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jodfedlet/TCC/blob/main/TCCII/Twitter/preprocessing_tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qZ61swqh5PS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Objetivos\n",
        "- Visualizar os tweets antes e depois do pré-processamento\n",
        "  - Quantidade de tweets - quantidade de plavras por tweets e média de palavras dos tweets geral/por ano e por mês\n",
        "- Pre-processar os tweets:\n",
        "  - remoção de stop words \n",
        "  - remoção de URLs\n",
        "  - conversão dos textos em letras minúsculas\n",
        "  - remoção de pontuação e acentuação\n",
        "  - remocação de palavras com menos:\n",
        "      - que iniciam com os caracteres @ ou #\n",
        "      - que possuem menos de 3 caracteres que são diferentes de ht ( sigla do país haiti)\n",
        "      - palavras que possuem mais de 3 occorências seguidas ( ex: kkkkkk)\n",
        "  - aplicação de bigrams ( técnica que faz com que palavras que aparecem juntas permaneçam juntas. Ex: Rio de Janeiro = rio_janeiro)\n",
        "  - lematização dos tweets (processo no qual é realizada uma análise morfológica das palavras, por\n",
        "exemplo palavras da terceira posição são alteradas para a primeira e verbos no passado e\n",
        "futuro transformados no presente)"
      ],
      "metadata": {
        "id": "7rBXGWYUMkQF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlCU2XyBh6Jp"
      },
      "source": [
        "###Importando/instalando bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qwmUtJFaqq5"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode\n",
        "!python3 -m spacy download pt_core_news_sm\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72EwE3OMOl0X",
        "outputId": "dee574b7-1c96-46b0-d19c-b3e41ad400ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all-corpora'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all-corpora\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import spacy\n",
        "import gensim\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "nltk.download('all-corpora')\n",
        "nltk.download('punkt')\n",
        "nltk.download('rslp')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk import regexp_tokenize\n",
        "from gensim.models import Phrases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR_wWs7KB_c1"
      },
      "source": [
        "# Carregamento dos tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "0KckrkjugjpP"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  tweets_orig = pd.read_csv('/content/drive/My Drive/Colab Notebooks/TCC/csv/all_tweets_with_retweets.csv', lineterminator='\\n', index_col=0)\n",
        "  # tweets_orig['date'] = pd.to_datetime(tweets_orig.datetime).dt.strftime(\"%Y-%m-%d\")\n",
        "  # tweets_orig = tweets_orig.set_index('date')\n",
        "  # tweets_orig = tweets_orig.drop(columns=\"datetime\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fHGAMSSMHCG",
        "outputId": "84e39837-1117-426c-edc5-00c6db3d9236"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(185348, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "tweets_orig.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_orig.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bOSe5whv5unp",
        "outputId": "7eb52942-468b-4370-c264-32bb2164da8b"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    datetime             tweet_id              user_id  \\\n",
              "0  2022-05-30 23:45:05+00:00  1531421471146393601  1217204834211958785   \n",
              "1  2022-05-30 23:00:53+00:00  1531410348141592577  1473798922472767488   \n",
              "2  2022-05-30 22:57:20+00:00  1531409451122577410  1521912766151417856   \n",
              "3  2022-05-30 22:55:14+00:00  1531408925945434112  1253882714723409920   \n",
              "4  2022-05-30 22:55:14+00:00  1531408923277807617             40320768   \n",
              "\n",
              "                                                text  \n",
              "0                        @geglobo Paquetá é haitiano  \n",
              "1  @BetoLuiz_RU @jonesmanoel_PCB @deccache O Hait...  \n",
              "2  O senhor vai investir na educação da Venezuela...  \n",
              "3  @rsallesmma Que nada, na mão de vocês já tá vi...  \n",
              "4  @JoppertLeonardo @jonesmanoel_PCB @deccache En...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36b808e5-2bb2-484c-980f-13fe6f3fb13d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-05-30 23:45:05+00:00</td>\n",
              "      <td>1531421471146393601</td>\n",
              "      <td>1217204834211958785</td>\n",
              "      <td>@geglobo Paquetá é haitiano</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-05-30 23:00:53+00:00</td>\n",
              "      <td>1531410348141592577</td>\n",
              "      <td>1473798922472767488</td>\n",
              "      <td>@BetoLuiz_RU @jonesmanoel_PCB @deccache O Hait...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-05-30 22:57:20+00:00</td>\n",
              "      <td>1531409451122577410</td>\n",
              "      <td>1521912766151417856</td>\n",
              "      <td>O senhor vai investir na educação da Venezuela...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-05-30 22:55:14+00:00</td>\n",
              "      <td>1531408925945434112</td>\n",
              "      <td>1253882714723409920</td>\n",
              "      <td>@rsallesmma Que nada, na mão de vocês já tá vi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-05-30 22:55:14+00:00</td>\n",
              "      <td>1531408923277807617</td>\n",
              "      <td>40320768</td>\n",
              "      <td>@JoppertLeonardo @jonesmanoel_PCB @deccache En...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36b808e5-2bb2-484c-980f-13fe6f3fb13d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36b808e5-2bb2-484c-980f-13fe6f3fb13d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36b808e5-2bb2-484c-980f-13fe6f3fb13d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhdF2XyEvwnY"
      },
      "source": [
        "#Estatística dos tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "gLr4BzuEvxXf"
      },
      "outputs": [],
      "source": [
        "def tweets_stats(df, option):\n",
        "  df['datetime'] = pd.to_datetime(df.datetime, format='%Y-%m-%d')\n",
        "\n",
        "  def get_df_len__count_words__mean(df):\n",
        "    count_all_tweets = len(df)\n",
        "\n",
        "    if option == 'pre':\n",
        "      df_text = df['text_preprocessed']\n",
        "    else:\n",
        "       df_text = df['text']\n",
        "\n",
        "    df['count_word'] = df_text.str.split().str.len()\n",
        "    count_all_words = df['count_word'].sum()\n",
        "    mean = round(count_all_words / count_all_tweets, 2)\n",
        "    return (df, count_all_tweets, count_all_words, mean)\n",
        "\n",
        "  #data by year\n",
        "  def plot_by_year(): \n",
        "    by_year = df.groupby([df['datetime'].dt.year.rename('Year')]).agg({'tweet_id':['count'], 'count_word': ['sum']}).reset_index()\n",
        "    by_year.columns = ['Year', 'Count tweets', 'Count words']\n",
        "    by_year = by_year.assign(Mean_words = lambda x: round(x['Count words'] / x['Count tweets'], 2))\n",
        "    print(by_year)\n",
        "\n",
        "  #data by 3 month\n",
        "  def quartely(): \n",
        "    trimestral = df.groupby(pd.Grouper(key='datetime', freq='3M', closed='left')).agg({'tweet_id':['count'], 'count_word': ['sum']}).reset_index()\n",
        "    trimestral.columns = ['Date', 'Count tweets', 'Count words']\n",
        "    trimestral = trimestral.assign(Mean_words = lambda x: round(x['Count words'] / x['Count tweets'], 2))\n",
        "    print(trimestral)\n",
        "\n",
        "  #by year month\n",
        "  def plot_by_year_and_month():\n",
        "    by_year_month = df.groupby([df['datetime'].dt.year.rename('Year'), df['datetime'].dt.month.rename('Month')]).agg({'tweet_id':['count'], 'count_word': ['sum']}).reset_index()\n",
        "    by_year_month.columns = ['Year', 'Month', 'Count tweets', 'Count words']\n",
        "    by_year_month = by_year_month.assign(Mean_words = lambda x: round(x['Count words'] / x['Count tweets'], 2))\n",
        "    print(by_year_month)\n",
        "   \n",
        "  df, df_len, count_words, mean_words  = get_df_len__count_words__mean(df)\n",
        "  # print('Total: {0}, Count word: {1}, Mean words: {2}'.format(df_len,count_words, mean_words))\n",
        "  # print()\n",
        "\n",
        "  print('*****Agrupado por ano****')\n",
        "  plot_by_year()\n",
        "  print('\\n')\n",
        "  \n",
        "  print('*****Agrupamento trimestral****')\n",
        "  quartely()\n",
        "  print('\\n')\n",
        "\n",
        "  print('*****Agrupado por mês e ano****')\n",
        "  plot_by_year_and_month()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7N_CmNr7JVo"
      },
      "source": [
        "###Estatística dos tweets antes do pré-processamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz9Mdnp7wCQI",
        "outputId": "2788e244-4449-4a29-f3a1-051bf8bc5ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Agrupado por ano****\n",
            "   Year  Count tweets  Count words  Mean_words\n",
            "0  2018         23691       531047       22.42\n",
            "1  2019         29739       742523       24.97\n",
            "2  2020         56375      1317064       23.36\n",
            "3  2021         62625      1621237       25.89\n",
            "4  2022         12918       330690       25.60\n",
            "\n",
            "\n",
            "*****Agrupamento trimestral****\n",
            "                        Date  Count tweets  Count words  Mean_words\n",
            "0  2018-03-31 00:00:00+00:00          6356       138675       21.82\n",
            "1  2018-06-30 00:00:00+00:00          5551       124146       22.36\n",
            "2  2018-09-30 00:00:00+00:00          5944       129531       21.79\n",
            "3  2018-12-31 00:00:00+00:00          5792       137499       23.74\n",
            "4  2019-03-31 00:00:00+00:00          8848       224848       25.41\n",
            "5  2019-06-30 00:00:00+00:00          6744       166749       24.73\n",
            "6  2019-09-30 00:00:00+00:00          6588       164614       24.99\n",
            "7  2019-12-31 00:00:00+00:00          7554       186289       24.66\n",
            "8  2020-03-31 00:00:00+00:00         28886       649652       22.49\n",
            "9  2020-06-30 00:00:00+00:00         13254       311383       23.49\n",
            "10 2020-09-30 00:00:00+00:00          7795       199771       25.63\n",
            "11 2020-12-31 00:00:00+00:00          6430       155751       24.22\n",
            "12 2021-03-31 00:00:00+00:00         13057       426344       32.65\n",
            "13 2021-06-30 00:00:00+00:00          8465       209052       24.70\n",
            "14 2021-09-30 00:00:00+00:00         33035       785919       23.79\n",
            "15 2021-12-31 00:00:00+00:00          8065       200054       24.81\n",
            "16 2022-03-31 00:00:00+00:00          7856       197877       25.19\n",
            "17 2022-06-30 00:00:00+00:00          5128       134407       26.21\n",
            "\n",
            "\n",
            "*****Agrupado por mês e ano****\n",
            "    Year  Month  Count tweets  Count words  Mean_words\n",
            "0   2018      1          2564        56315       21.96\n",
            "1   2018      2          2525        54548       21.60\n",
            "2   2018      3          1300        28565       21.97\n",
            "3   2018      4          1298        29096       22.42\n",
            "4   2018      5          2613        57054       21.83\n",
            "5   2018      6          1634        37871       23.18\n",
            "6   2018      7          2412        48864       20.26\n",
            "7   2018      8          1978        44443       22.47\n",
            "8   2018      9          1558        36399       23.36\n",
            "9   2018     10          1909        43484       22.78\n",
            "10  2018     11          2170        55271       25.47\n",
            "11  2018     12          1730        39137       22.62\n",
            "12  2019      1          2532        65964       26.05\n",
            "13  2019      2          4208       105975       25.18\n",
            "14  2019      3          2106        52913       25.12\n",
            "15  2019      4          1829        46041       25.17\n",
            "16  2019      5          1772        45288       25.56\n",
            "17  2019      6          3417        81326       23.80\n",
            "18  2019      7          1938        48051       24.79\n",
            "19  2019      8          2078        54021       26.00\n",
            "20  2019      9          2299        56664       24.65\n",
            "21  2019     10          2630        65245       24.81\n",
            "22  2019     11          2872        71319       24.83\n",
            "23  2019     12          2058        49716       24.16\n",
            "24  2020      1          3992        97314       24.38\n",
            "25  2020      2          3189        75608       23.71\n",
            "26  2020      3         21886       481147       21.98\n",
            "27  2020      4          4797       100957       21.05\n",
            "28  2020      5          4852       117714       24.26\n",
            "29  2020      6          3441        88949       25.85\n",
            "30  2020      7          2372        62103       26.18\n",
            "31  2020      8          2395        61781       25.80\n",
            "32  2020      9          3017        75591       25.06\n",
            "33  2020     10          2257        54343       24.08\n",
            "34  2020     11          2083        49910       23.96\n",
            "35  2020     12          2094        51647       24.66\n",
            "36  2021      1          4691       163826       34.92\n",
            "37  2021      2          4306       141194       32.79\n",
            "38  2021      3          4111       122626       29.83\n",
            "39  2021      4          2818        69243       24.57\n",
            "40  2021      5          2111        55012       26.06\n",
            "41  2021      6          3576        84692       23.68\n",
            "42  2021      7         16628       399342       24.02\n",
            "43  2021      8         11876       271635       22.87\n",
            "44  2021      9          4489       114690       25.55\n",
            "45  2021     10          3010        73709       24.49\n",
            "46  2021     11          2251        58409       25.95\n",
            "47  2021     12          2758        66859       24.24\n",
            "48  2022      1          2469        62022       25.12\n",
            "49  2022      2          3010        72629       24.13\n",
            "50  2022      3          2388        63724       26.69\n",
            "51  2022      4          2220        57334       25.83\n",
            "52  2022      5          2831        74981       26.49\n"
          ]
        }
      ],
      "source": [
        "tweets_stats(tweets_orig, 'ori')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgOt6eY9GGIN"
      },
      "source": [
        "#Pegar os stop words da língua portuguesa usando NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "MD9FttR_ZujT"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('portuguese'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LItshnuibmxO",
        "outputId": "187376a4-b2a9-4fca-af9e-fe45de41c3c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "type(stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxY7V9fec_-4",
        "outputId": "7935b8f0-702c-458e-c746-bbfdf0f9728f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'ao',\n",
              " 'aos',\n",
              " 'aquela',\n",
              " 'aquelas',\n",
              " 'aquele',\n",
              " 'aqueles',\n",
              " 'aquilo',\n",
              " 'as',\n",
              " 'até',\n",
              " 'com',\n",
              " 'como',\n",
              " 'da',\n",
              " 'das',\n",
              " 'de',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'dele',\n",
              " 'deles',\n",
              " 'depois',\n",
              " 'do',\n",
              " 'dos',\n",
              " 'e',\n",
              " 'ela',\n",
              " 'elas',\n",
              " 'ele',\n",
              " 'eles',\n",
              " 'em',\n",
              " 'entre',\n",
              " 'era',\n",
              " 'eram',\n",
              " 'essa',\n",
              " 'essas',\n",
              " 'esse',\n",
              " 'esses',\n",
              " 'esta',\n",
              " 'estamos',\n",
              " 'estas',\n",
              " 'estava',\n",
              " 'estavam',\n",
              " 'este',\n",
              " 'esteja',\n",
              " 'estejam',\n",
              " 'estejamos',\n",
              " 'estes',\n",
              " 'esteve',\n",
              " 'estive',\n",
              " 'estivemos',\n",
              " 'estiver',\n",
              " 'estivera',\n",
              " 'estiveram',\n",
              " 'estiverem',\n",
              " 'estivermos',\n",
              " 'estivesse',\n",
              " 'estivessem',\n",
              " 'estivéramos',\n",
              " 'estivéssemos',\n",
              " 'estou',\n",
              " 'está',\n",
              " 'estávamos',\n",
              " 'estão',\n",
              " 'eu',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'for',\n",
              " 'fora',\n",
              " 'foram',\n",
              " 'forem',\n",
              " 'formos',\n",
              " 'fosse',\n",
              " 'fossem',\n",
              " 'fui',\n",
              " 'fôramos',\n",
              " 'fôssemos',\n",
              " 'haja',\n",
              " 'hajam',\n",
              " 'hajamos',\n",
              " 'havemos',\n",
              " 'hei',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houver',\n",
              " 'houvera',\n",
              " 'houveram',\n",
              " 'houverei',\n",
              " 'houverem',\n",
              " 'houveremos',\n",
              " 'houveria',\n",
              " 'houveriam',\n",
              " 'houvermos',\n",
              " 'houverá',\n",
              " 'houverão',\n",
              " 'houveríamos',\n",
              " 'houvesse',\n",
              " 'houvessem',\n",
              " 'houvéramos',\n",
              " 'houvéssemos',\n",
              " 'há',\n",
              " 'hão',\n",
              " 'isso',\n",
              " 'isto',\n",
              " 'já',\n",
              " 'lhe',\n",
              " 'lhes',\n",
              " 'mais',\n",
              " 'mas',\n",
              " 'me',\n",
              " 'mesmo',\n",
              " 'meu',\n",
              " 'meus',\n",
              " 'minha',\n",
              " 'minhas',\n",
              " 'muito',\n",
              " 'na',\n",
              " 'nas',\n",
              " 'nem',\n",
              " 'no',\n",
              " 'nos',\n",
              " 'nossa',\n",
              " 'nossas',\n",
              " 'nosso',\n",
              " 'nossos',\n",
              " 'num',\n",
              " 'numa',\n",
              " 'não',\n",
              " 'nós',\n",
              " 'o',\n",
              " 'os',\n",
              " 'ou',\n",
              " 'para',\n",
              " 'pela',\n",
              " 'pelas',\n",
              " 'pelo',\n",
              " 'pelos',\n",
              " 'por',\n",
              " 'qual',\n",
              " 'quando',\n",
              " 'que',\n",
              " 'quem',\n",
              " 'se',\n",
              " 'seja',\n",
              " 'sejam',\n",
              " 'sejamos',\n",
              " 'sem',\n",
              " 'serei',\n",
              " 'seremos',\n",
              " 'seria',\n",
              " 'seriam',\n",
              " 'será',\n",
              " 'serão',\n",
              " 'seríamos',\n",
              " 'seu',\n",
              " 'seus',\n",
              " 'somos',\n",
              " 'sou',\n",
              " 'sua',\n",
              " 'suas',\n",
              " 'são',\n",
              " 'só',\n",
              " 'também',\n",
              " 'te',\n",
              " 'tem',\n",
              " 'temos',\n",
              " 'tenha',\n",
              " 'tenham',\n",
              " 'tenhamos',\n",
              " 'tenho',\n",
              " 'terei',\n",
              " 'teremos',\n",
              " 'teria',\n",
              " 'teriam',\n",
              " 'terá',\n",
              " 'terão',\n",
              " 'teríamos',\n",
              " 'teu',\n",
              " 'teus',\n",
              " 'teve',\n",
              " 'tinha',\n",
              " 'tinham',\n",
              " 'tive',\n",
              " 'tivemos',\n",
              " 'tiver',\n",
              " 'tivera',\n",
              " 'tiveram',\n",
              " 'tiverem',\n",
              " 'tivermos',\n",
              " 'tivesse',\n",
              " 'tivessem',\n",
              " 'tivéramos',\n",
              " 'tivéssemos',\n",
              " 'tu',\n",
              " 'tua',\n",
              " 'tuas',\n",
              " 'tém',\n",
              " 'tínhamos',\n",
              " 'um',\n",
              " 'uma',\n",
              " 'você',\n",
              " 'vocês',\n",
              " 'vos',\n",
              " 'à',\n",
              " 'às',\n",
              " 'é',\n",
              " 'éramos'}"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM-vll20KqvP"
      },
      "source": [
        "#Pré-processamento dos tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAlOZbaXCNDM"
      },
      "source": [
        "###Função para limpar os tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "wKc3KHEUkiTm"
      },
      "outputs": [],
      "source": [
        "def cleaning(tweet):\n",
        "  \n",
        "  #remover pontuações\n",
        "  def remove_punctuation(tweet):\n",
        "      return re.sub(r'[^\\w\\s]','',tweet) \n",
        "\n",
        "  #remocao de plavras com mais de 3 ocorrencias seguidas\n",
        "  def mais_de_3_ocorrencias_seguidas(word):  \n",
        "    let_ant = word[0]\n",
        "    count = 0\n",
        "    mais_de_3 = False;\n",
        "    for w in word:\n",
        "      count = count + 1 if w == let_ant else 1\n",
        "      let_ant = w  \n",
        "      if count > 3: \n",
        "        mais_de_3 = True \n",
        "        break  \n",
        "    return mais_de_3   \n",
        "\n",
        "  #remover acentuações\n",
        "  def remove_emphasis(token):\n",
        "        return unidecode(remove_punctuation(token))\n",
        "\n",
        "  stop_words_list = [remove_emphasis(st_word) for st_word in stop_words]\n",
        " \n",
        "  #conversão de string para minúsculas, remoção de pontuação, palavras que iniciam com @, #, stopwords ou com tamanho < 2 e se forem diferentes de ht.\n",
        "  def remove_undesirable_words(tweet):\n",
        "      new_list_ = []\n",
        "      tweet = ' '.join(str(word).lower() for word in tweet.split() if not word.startswith('@') and not word.startswith('#'))\n",
        "     \n",
        "      for token in wordpunct_tokenize(tweet):\n",
        "         if token in stop_words_list or (len(token) < 3 and token != 'ht') or mais_de_3_ocorrencias_seguidas(token): continue\n",
        "         new_list_.append(remove_emphasis(token))\n",
        "      return ' '.join(new_list_)\n",
        "\n",
        "  #remover urls\n",
        "  def remove_url(tweet_without_sw):\n",
        "      return re.sub(r\"http\\S+\", \"\", tweet_without_sw)\n",
        "\n",
        "  #remover palavras que contem ou que são números\n",
        "  def remove_numeric(tweet):\n",
        "    return re.sub(r'\\d+', '', tweet)\n",
        "\n",
        "  tweet = remove_url(tweet)\n",
        "  tweet = remove_numeric(tweet)\n",
        "  tweet = remove_undesirable_words(tweet)\n",
        "  return tweet \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3NOY8WQEMH7"
      },
      "source": [
        "\n",
        "###Exemplo de tweet antes da aplicação da função de limpeza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "mMmawXRUfeGY"
      },
      "outputs": [],
      "source": [
        "string_example = \"No Rio de Janeiro, em 2021, <p> o nao @senhor vai #investir, em 2022, na educação da Venezuela, Argentina, Haiti,cuba.e etc países comunistas?, porque aqui no Brasil o Senhor nunca fez isso ?????. https://t.co/jmVSYbsG2X\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "2cPuLlD1dxF7"
      },
      "outputs": [],
      "source": [
        "filtered_tweet = cleaning(string_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL-GiFlgEelV"
      },
      "source": [
        "###Exemplo de tweet depois da aplicação da função de limpeza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "onsjf2SAWsC0",
        "outputId": "49578c53-ccd3-40d1-c421-3c954f040df1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rio janeiro vai educacao venezuela argentina haiti cuba etc paises comunistas porque aqui brasil senhor nunca fez'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "filtered_tweet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMjfv4xtFeF6"
      },
      "source": [
        "###Aplicação da função de limpeza dos tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "0f-PAjiRe0S0"
      },
      "outputs": [],
      "source": [
        "tweets_pre_proc = tweets_orig.copy()\n",
        "tweets_pre_proc['text_preprocessed'] = tweets_orig['text'].map(lambda tweet: cleaning(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8uljx9xYuwY",
        "outputId": "e3c15067-bd36-4c08-cba7-8ce3fb0b1e70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@geglobo Paquetá é haitiano',\n",
              " '@BetoLuiz_RU @jonesmanoel_PCB @deccache O Haiti foi uma desgraça de fato.Mas os erros nao anulam os acertos.E quando ele fala sobre a guerra na Ucrania ele vem se posicionando muito bem,criticando a postura do zelensky',\n",
              " 'O senhor vai investir na educação da Venezuela, Argentina, Haiti,cuba, e etc países comunistas?, porque aqui no Brasil o Senhor nunca fez isso. https://t.co/jmVSYbsG2X',\n",
              " '@rsallesmma Que nada, na mão de vocês já tá virando uma mistura de deserto do Saara com Haiti',\n",
              " '@JoppertLeonardo @jonesmanoel_PCB @deccache Entramos nos BRICS, mas invadimos o Haiti a mando do Bush!\\n\\nLula diz que vai fortalecer os brics mas faz palestras no Parlamento Europeu que está tratando Rússia e china como inimigos mortais, como isso vai funcionar???']"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "tweets_orig.text.head().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGbjhqfmzDFQ",
        "outputId": "951c3876-a25c-45b5-e7ec-e9804c8b4bb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paqueta haitiano',\n",
              " 'haiti desgraca fato erros anulam acertos fala sobre guerra ucrania vem posicionando bem criticando postura zelensky',\n",
              " 'senhor vai investir educacao venezuela argentina haiti cuba etc paises comunistas porque aqui brasil senhor nunca fez',\n",
              " 'nada mao voces virando mistura deserto saara haiti',\n",
              " 'entramos brics invadimos haiti mando bush lula diz vai fortalecer brics faz palestras parlamento europeu esta tratando russia china inimigos mortais vai funcionar ']"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "tweets_pre_proc.text_preprocessed.head().tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_A3UZOKMVoD"
      },
      "source": [
        "###Remover tweets com menos de 3 palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "9AddF7arLr7F"
      },
      "outputs": [],
      "source": [
        "tweets_pre_proc = tweets_pre_proc.loc[tweets_pre_proc['text_preprocessed'].str.split().str.len() > 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGzg91V6L-N7",
        "outputId": "4b6a3441-52e3-4cf8-e2dd-8832811e2a19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(171087, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "tweets_pre_proc.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAADO70qDBO9"
      },
      "source": [
        "###Remover os tweets duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "51YZj0HF-VkT"
      },
      "outputs": [],
      "source": [
        "tweets_pre_proc.drop_duplicates(subset = [\"text_preprocessed\"], keep='first', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZaDi_AFFL5b"
      },
      "source": [
        "###Quantidade de tweets após a remoção dos tweets duplicados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IC5wWgm-dnl",
        "outputId": "e4658487-f282-4e6f-c5fc-760b5728bbc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150264, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "tweets_pre_proc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paxssE2-L7WK",
        "outputId": "a217d3fd-4b11-46dd-cb68-5b65c57f3425"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@geglobo Paquetá é haitiano',\n",
              " '@BetoLuiz_RU @jonesmanoel_PCB @deccache O Haiti foi uma desgraça de fato.Mas os erros nao anulam os acertos.E quando ele fala sobre a guerra na Ucrania ele vem se posicionando muito bem,criticando a postura do zelensky',\n",
              " 'O senhor vai investir na educação da Venezuela, Argentina, Haiti,cuba, e etc países comunistas?, porque aqui no Brasil o Senhor nunca fez isso. https://t.co/jmVSYbsG2X',\n",
              " '@rsallesmma Que nada, na mão de vocês já tá virando uma mistura de deserto do Saara com Haiti',\n",
              " '@JoppertLeonardo @jonesmanoel_PCB @deccache Entramos nos BRICS, mas invadimos o Haiti a mando do Bush!\\n\\nLula diz que vai fortalecer os brics mas faz palestras no Parlamento Europeu que está tratando Rússia e china como inimigos mortais, como isso vai funcionar???']"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ],
      "source": [
        "tweets_orig.text.head().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_WQhu4WXc8x",
        "outputId": "896682b0-2ff9-4a51-b40f-a9fd3c73c07f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haiti desgraca fato erros anulam acertos fala sobre guerra ucrania vem posicionando bem criticando postura zelensky',\n",
              " 'senhor vai investir educacao venezuela argentina haiti cuba etc paises comunistas porque aqui brasil senhor nunca fez',\n",
              " 'nada mao voces virando mistura deserto saara haiti',\n",
              " 'entramos brics invadimos haiti mando bush lula diz vai fortalecer brics faz palestras parlamento europeu esta tratando russia china inimigos mortais vai funcionar ',\n",
              " 'haitiano entendi nada falavam']"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "tweets_pre_proc.text_preprocessed.head().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_YlFRbv8duT"
      },
      "source": [
        "###Estatística dos tweets após o pré-processamento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOCGonzX8Q2l",
        "outputId": "a791c0c7-1c68-4f7a-d4a2-e67eb8add1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Agrupado por ano****\n",
            "   Year  Count tweets  Count words  Mean_words\n",
            "0  2018         18202       247212       13.58\n",
            "1  2019         24449       364905       14.93\n",
            "2  2020         47530       650724       13.69\n",
            "3  2021         48733       704287       14.45\n",
            "4  2022         11350       170537       15.03\n",
            "\n",
            "\n",
            "*****Agrupamento trimestral****\n",
            "                        Date  Count tweets  Count words  Mean_words\n",
            "0  2018-03-31 00:00:00+00:00          4796        63502       13.24\n",
            "1  2018-06-30 00:00:00+00:00          4443        58990       13.28\n",
            "2  2018-09-30 00:00:00+00:00          4574        61020       13.34\n",
            "3  2018-12-31 00:00:00+00:00          4349        63126       14.52\n",
            "4  2019-03-31 00:00:00+00:00          7341       111249       15.15\n",
            "5  2019-06-30 00:00:00+00:00          5431        80881       14.89\n",
            "6  2019-09-30 00:00:00+00:00          5620        82266       14.64\n",
            "7  2019-12-31 00:00:00+00:00          6059        90544       14.94\n",
            "8  2020-03-31 00:00:00+00:00         24600       318557       12.95\n",
            "9  2020-06-30 00:00:00+00:00         11099       154055       13.88\n",
            "10 2020-09-30 00:00:00+00:00          6610       100844       15.26\n",
            "11 2020-12-31 00:00:00+00:00          5200        76870       14.78\n",
            "12 2021-03-31 00:00:00+00:00          7155       106364       14.87\n",
            "13 2021-06-30 00:00:00+00:00          7035       102001       14.50\n",
            "14 2021-09-30 00:00:00+00:00         27699       395139       14.27\n",
            "15 2021-12-31 00:00:00+00:00          6840       100870       14.75\n",
            "16 2022-03-31 00:00:00+00:00          6878       102468       14.90\n",
            "17 2022-06-30 00:00:00+00:00          4535        68919       15.20\n",
            "\n",
            "\n",
            "*****Agrupado por mês e ano****\n",
            "    Year  Month  Count tweets  Count words  Mean_words\n",
            "0   2018      1          1957        25945       13.26\n",
            "1   2018      2          1836        24396       13.29\n",
            "2   2018      3          1026        13485       13.14\n",
            "3   2018      4           984        13494       13.71\n",
            "4   2018      5          2095        26978       12.88\n",
            "5   2018      6          1366        18525       13.56\n",
            "6   2018      7          1861        23147       12.44\n",
            "7   2018      8          1520        21052       13.85\n",
            "8   2018      9          1195        16893       14.14\n",
            "9   2018     10          1412        19919       14.11\n",
            "10  2018     11          1713        26193       15.29\n",
            "11  2018     12          1237        17185       13.89\n",
            "12  2019      1          2089        32232       15.43\n",
            "13  2019      2          3553        53191       14.97\n",
            "14  2019      3          1700        25849       15.21\n",
            "15  2019      4          1499        22732       15.16\n",
            "16  2019      5          1487        22476       15.11\n",
            "17  2019      6          2670        38608       14.46\n",
            "18  2019      7          1691        24307       14.37\n",
            "19  2019      8          1786        27051       15.15\n",
            "20  2019      9          1919        28016       14.60\n",
            "21  2019     10          2175        32688       15.03\n",
            "22  2019     11          2327        34615       14.88\n",
            "23  2019     12          1553        23140       14.90\n",
            "24  2020      1          3304        48532       14.69\n",
            "25  2020      2          2663        36309       13.63\n",
            "26  2020      3         18799       236040       12.56\n",
            "27  2020      4          3961        49584       12.52\n",
            "28  2020      5          4065        57434       14.13\n",
            "29  2020      6          2930        45138       15.41\n",
            "30  2020      7          2029        31445       15.50\n",
            "31  2020      8          2070        31678       15.30\n",
            "32  2020      9          2501        37561       15.02\n",
            "33  2020     10          1815        26836       14.79\n",
            "34  2020     11          1690        25142       14.88\n",
            "35  2020     12          1703        25025       14.69\n",
            "36  2021      1          2188        32256       14.74\n",
            "37  2021      2          2301        34808       15.13\n",
            "38  2021      3          2683        39431       14.70\n",
            "39  2021      4          2303        33266       14.44\n",
            "40  2021      5          1827        28013       15.33\n",
            "41  2021      6          2947        41060       13.93\n",
            "42  2021      7         13904       200485       14.42\n",
            "43  2021      8          9932       136457       13.74\n",
            "44  2021      9          3833        58098       15.16\n",
            "45  2021     10          2493        37122       14.89\n",
            "46  2021     11          1943        29404       15.13\n",
            "47  2021     12          2379        33887       14.24\n",
            "48  2022      1          2112        31633       14.98\n",
            "49  2022      2          2643        37780       14.29\n",
            "50  2022      3          2130        33234       15.60\n",
            "51  2022      4          1968        29633       15.06\n",
            "52  2022      5          2497        38257       15.32\n"
          ]
        }
      ],
      "source": [
        "tweets_stats(tweets_pre_proc, 'pre')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yM03oGL-Jxo"
      },
      "source": [
        "###Lematização e stemização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "CnC_XX9LADcR"
      },
      "outputs": [],
      "source": [
        "pln = spacy.load('pt_core_news_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bigramas"
      ],
      "metadata": {
        "id": "d-BmtT83MoEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bigrams(df, option):\n",
        "  data = df.text_lematizados if option =='lem' else df.text_stematizados\n",
        "  docs = [doc.split() for doc in data]\n",
        "  bigram = Phrases(docs, min_count=3)\n",
        "  for i in range(len(docs)):\n",
        "    docs[i] = ' '.join(bigram[docs[i]])\n",
        "  return docs  "
      ],
      "metadata": {
        "id": "q43bsqq9MfQF"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBsgn00LCmgR"
      },
      "source": [
        "###Lematização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "_ZJ0sYt6AS_2"
      },
      "outputs": [],
      "source": [
        "string_lem_example = 'lixo tiver nada sou capacetes brancos ajudaram haiti pare besteira ajudou haiti fim exercito capacetes brancos resolvem problema presos campos concentracao argentina criou'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "KPKFg1c2-IMC"
      },
      "outputs": [],
      "source": [
        "def lemmatization(tweet):\n",
        "  document = pln(tweet)\n",
        "  return ' '.join([str(token.lemma_).lower() for token in document])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "AmCCuTS0AlRp"
      },
      "outputs": [],
      "source": [
        "tweet_lem_example = lemmatization(string_lem_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3mXcYsiFCwZu",
        "outputId": "5706e46a-7a94-485c-ad83-0de85caaf206"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'lixo ter nada ser capacete branco ajudar Haiti pare besteira ajudar Haiti fim exercito capacete branco resolver problema prender Campos concentracao Argentina criar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "tweet_lem_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "UOW43XYZDnTo"
      },
      "outputs": [],
      "source": [
        "tweets_pre_proc['text_lematizados'] = tweets_pre_proc['text_preprocessed'].map(lambda tweet: lemmatization(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "2dP-qxwMD8ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83126bf0-5ebe-4c4b-fcb4-aac417843e83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haiti desgracar fato erro anular acerto falar sobre guerra ucrania vir posicionar bem criticar postura zelensky',\n",
              " 'senhor ir investir educacao venezuela argentina haiti cuba etc paise comunista porque aqui brasil senhor nunca fazer',\n",
              " 'nada mao voce virar mistura deserto saara haiti',\n",
              " 'entra brics invadir haiti mar bush lula dizer ir fortalecer brics fazer palestra parlamento europeu este tratar russia china inimigo mortal ir funcionar',\n",
              " 'haitiano entendi nada falar']"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ],
      "source": [
        "tweets_pre_proc.text_lematizados.head().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_pre_proc['bigrams_lem'] = bigrams(tweets_pre_proc, 'lem')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoh6Ljw10OCh",
        "outputId": "7798faf5-f0d0-40ec-d6cd-f8199e6d3dd6"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_pre_proc.bigrams_lem.head().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gW87laeLEF3",
        "outputId": "a4277fe2-7225-4d5d-9142-713a2bb9eb35"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haiti desgracar fato erro anular acerto falar sobre guerra_ucrania vir posicionar bem criticar postura zelensky',\n",
              " 'senhor ir investir_educacao venezuela argentina haiti cuba etc paise comunista porque aqui brasil senhor nunca fazer',\n",
              " 'nada mao voce virar_mistura deserto saara haiti',\n",
              " 'entra brics invadir haiti mar_bush lula dizer ir fortalecer brics fazer palestra parlamento europeu este tratar russia_china inimigo mortal ir funcionar',\n",
              " 'haitiano entendi_nada falar']"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDr540tzEMcg"
      },
      "source": [
        "###Stematização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "spEstnAEEPa4"
      },
      "outputs": [],
      "source": [
        "string_stem_example = 'capacetes brancos ajudaram haiti pare besteira ajudou haiti fim exercito capacetes brancos resolvem problema presos campos concentracao argentina criou'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "sxGZ0sKtErVc"
      },
      "outputs": [],
      "source": [
        "def stemming(tweet):\n",
        "  document = pln(tweet)\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  return ' '.join([str(stemmer.stem(token.text)).lower() for token in document])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "E5Wxc-D_Ex4_"
      },
      "outputs": [],
      "source": [
        "string_stem_example = stemming(string_stem_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "TcSVDRNCE4RQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aca7f3b1-e536-4959-ae35-243b790c3520"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'capacet branc ajud hait par beste ajud hait fim exercit capacet branc resolv problem pres camp concentraca argentin cri'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "string_stem_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "THBmoMozOj9I"
      },
      "outputs": [],
      "source": [
        "tweets_pre_proc['text_stematizados'] = tweets_pre_proc['text_preprocessed'].map(lambda tweet: stemming(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "xMDljXZzO6Mg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d223882d-e885-4858-cadb-98a935ec12ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hait desgrac fat err anul acert fal sobr guerr ucran vem posicion bem critic post zelensky',\n",
              " 'senh vai invest educaca venezuel argentin hait cub etc pais comun porqu aqu brasil senh nunc fez',\n",
              " 'nad mao voc vir mist desert sa hait',\n",
              " 'entr bric invad hait mand bush lul diz vai fortalec bric faz palestr parl europ est trat russ chin inimig mort vai funcion',\n",
              " 'haiti entend nad fal']"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ],
      "source": [
        "tweets_pre_proc.text_stematizados.head().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_pre_proc['bigrams_stem'] = bigrams(tweets_pre_proc,'stem')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63Y0P7a10dda",
        "outputId": "22efb6f5-e902-44fe-b7b2-7bbe57878d86"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_pre_proc.bigrams_stem.head().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8oB-k-Y2xBI",
        "outputId": "0c0a0148-8557-4731-f5c9-299b204a91eb"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hait desgrac fat err anul acert fal sobr guerr_ucran vem posicion bem critic post zelensky',\n",
              " 'senh vai invest_educaca venezuel argentin hait cub etc pais comun porqu aqu brasil senh nunc fez',\n",
              " 'nad mao voc vir_mist desert_sa hait',\n",
              " 'entr bric invad hait mand bush lul diz vai fortalec bric faz palestr parl europ est trat russ_chin inimig mort vai funcion',\n",
              " 'haiti entend_nad fal']"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnRiIaCuWFmn"
      },
      "source": [
        "#Comparação de tweets originais, pré-processados, lematizados, **bigramados e **stematizados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhsrDBUGWUX4"
      },
      "source": [
        "###Tweets originais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "mbsDEFUbWEpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfa40d8-a070-4232-9d11-028b6e02d1b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@geglobo Paquetá é haitiano',\n",
              " '@BetoLuiz_RU @jonesmanoel_PCB @deccache O Haiti foi uma desgraça de fato.Mas os erros nao anulam os acertos.E quando ele fala sobre a guerra na Ucrania ele vem se posicionando muito bem,criticando a postura do zelensky',\n",
              " 'O senhor vai investir na educação da Venezuela, Argentina, Haiti,cuba, e etc países comunistas?, porque aqui no Brasil o Senhor nunca fez isso. https://t.co/jmVSYbsG2X',\n",
              " '@rsallesmma Que nada, na mão de vocês já tá virando uma mistura de deserto do Saara com Haiti',\n",
              " '@JoppertLeonardo @jonesmanoel_PCB @deccache Entramos nos BRICS, mas invadimos o Haiti a mando do Bush!\\n\\nLula diz que vai fortalecer os brics mas faz palestras no Parlamento Europeu que está tratando Rússia e china como inimigos mortais, como isso vai funcionar???']"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "tweets_orig.text.head().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwCswcJ2Weai"
      },
      "source": [
        "### Tweets pré-processados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "v_9v4lKcWdX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddc8f21-35d2-4d31-a37d-679fd6e0d0de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haiti desgraca fato erros anulam acertos fala sobre guerra ucrania vem posicionando bem criticando postura zelensky',\n",
              " 'senhor vai investir educacao venezuela argentina haiti cuba etc paises comunistas porque aqui brasil senhor nunca fez',\n",
              " 'nada mao voces virando mistura deserto saara haiti',\n",
              " 'entramos brics invadimos haiti mando bush lula diz vai fortalecer brics faz palestras parlamento europeu esta tratando russia china inimigos mortais vai funcionar ',\n",
              " 'haitiano entendi nada falavam']"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "tweets_pre_proc.text_preprocessed.head().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxW4UASqWmpo"
      },
      "source": [
        "###Tweets lematizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "XZWfGcaFWmKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd0f09b-b0e5-4bea-f56a-0138a29b896a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haiti desgracar fato erro anular acerto falar sobre guerra ucrania vir posicionar bem criticar postura zelensky',\n",
              " 'senhor ir investir educacao venezuela argentina haiti cuba etc paise comunista porque aqui brasil senhor nunca fazer',\n",
              " 'nada mao voce virar mistura deserto saara haiti',\n",
              " 'entra brics invadir haiti mar bush lula dizer ir fortalecer brics fazer palestra parlamento europeu este tratar russia china inimigo mortal ir funcionar',\n",
              " 'haitiano entendi nada falar']"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "source": [
        "tweets_pre_proc.text_lematizados.head().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tweets com bigramas"
      ],
      "metadata": {
        "id": "iG9csU4bL5hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_pre_proc.bigrams_lem.head().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOFSwjkdL4se",
        "outputId": "8cb9d93a-d869-4788-e050-11b7ba19690c"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['haiti desgracar fato erro anular acerto falar sobre guerra_ucrania vir posicionar bem criticar postura zelensky',\n",
              " 'senhor ir investir_educacao venezuela argentina haiti cuba etc paise comunista porque aqui brasil senhor nunca fazer',\n",
              " 'nada mao voce virar_mistura deserto saara haiti',\n",
              " 'entra brics invadir haiti mar_bush lula dizer ir fortalecer brics fazer palestra parlamento europeu este tratar russia_china inimigo mortal ir funcionar',\n",
              " 'haitiano entendi_nada falar']"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edOZm1e_WzDB"
      },
      "source": [
        "###Tweets stematizados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "VIAbuvtZWwxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ac5ff8-53c0-4b18-bc92-4ccc29eb5b5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hait desgrac fat err anul acert fal sobr guerr ucran vem posicion bem critic post zelensky',\n",
              " 'senh vai invest educaca venezuel argentin hait cub etc pais comun porqu aqu brasil senh nunc fez',\n",
              " 'nad mao voc vir mist desert sa hait',\n",
              " 'entr bric invad hait mand bush lul diz vai fortalec bric faz palestr parl europ est trat russ chin inimig mort vai funcion',\n",
              " 'haiti entend nad fal']"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ],
      "source": [
        "tweets_pre_proc.text_stematizados.head().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI_Fn8Q8hiwE"
      },
      "source": [
        "##Criando o arquivo .csv dos tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "REQ5FB9hHOtE"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(tweets_pre_proc)\n",
        "df.to_csv('/content/drive/My Drive/Colab Notebooks/TCC/csv/pre_processed_tweets.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocessing_tweets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOf05LBOW9mGv6GjQTRJynv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}